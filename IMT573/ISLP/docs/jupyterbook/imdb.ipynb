{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f2b809",
   "metadata": {},
   "source": [
    "# Creating IMDB dataset from `keras` version\n",
    "\n",
    "This script details how the `IMDB` data in `ISLP` was constructed.\n",
    "\n",
    "Running this example requires `keras`. Use `pip install keras` to install if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d920bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, save_npz\n",
    "import torch\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf27f0c-0cb0-4ad5-8775-d138e3f20933",
   "metadata": {},
   "source": [
    "We first load the data using `keras`, limiting focus to the 10000 most commmon words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f0e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 3 is for three terms: <START> <UNK> <UNUSED> \n",
    "num_words = 10000+3\n",
    "((S_train, L_train), \n",
    " (S_test, L_test)) = imdb.load_data(num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9020ab27-cc62-4b86-85ba-80a94ff692de",
   "metadata": {},
   "source": [
    "The object `S_train` is effectively a list in which each document has been encoded into a sequence of\n",
    "values from 0 to 10002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e27564c4-320f-42b6-9f2e-2a2afdebefcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_train[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f039fe-faed-4884-a725-1c51d6c8d4d4",
   "metadata": {},
   "source": [
    "We'll use `np.float32` as that is the common precision used in `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc3c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train = L_train.astype(np.float32)\n",
    "L_test = L_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005679bc-4337-4757-831e-f9a6ea50f6aa",
   "metadata": {},
   "source": [
    "We will use a one-hot encoding that captures whether or not a given word appears in a given review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b6d1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(sequences, ncol):\n",
    "    idx, vals = [], []\n",
    "    for i, s in enumerate(sequences):\n",
    "        idx.extend({(i,v):1 for v in s}.keys())\n",
    "    idx = np.array(idx).T\n",
    "    vals = np.ones(idx.shape[1], dtype=np.float32)\n",
    "    tens = torch.sparse_coo_tensor(indices=idx,\n",
    "                                   values=vals,\n",
    "                                   size=(len(sequences), ncol))\n",
    "    return tens.coalesce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afcdc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = one_hot(S_train, num_words)\n",
    "X_test = one_hot(S_test, num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67e299d-8774-4758-8953-77afdce775ab",
   "metadata": {},
   "source": [
    "## Store as sparse tensors\n",
    "\n",
    "We see later in the lab that the dense representation is faster. Nevertheless,\n",
    "let's store the one-hot representation as sparse `torch` tensors \n",
    "as well as sparse `scipy` matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19366ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_tensor(X):\n",
    "    idx = np.asarray(X.indices())\n",
    "    vals = np.asarray(X.values())\n",
    "    return coo_matrix((vals,\n",
    "                      (idx[0],\n",
    "                       idx[1])),\n",
    "                      shape=X.shape).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b45ae6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s = convert_sparse_tensor(X_train)\n",
    "X_test_s = convert_sparse_tensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a47d6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d = torch.tensor(X_train_s.todense())\n",
    "X_test_d = torch.tensor(X_test_s.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b37b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_train_d, 'IMDB_X_train.tensor')\n",
    "torch.save(X_test_d, 'IMDB_X_test.tensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119823a",
   "metadata": {},
   "source": [
    "### Save as sparse `scipy` matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb6bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('IMDB_X_test.npz', X_test_s)\n",
    "save_npz('IMDB_X_train.npz', X_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eac1c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('IMDB_Y_test.npy', L_test)\n",
    "np.save('IMDB_Y_train.npy', L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c128e3",
   "metadata": {},
   "source": [
    "## Save and pickle the word index\n",
    "\n",
    "We'll also want to store a lookup table to convert representations such as `S_train[0]` into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8458bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "lookup = {(i+3):w for w, i in word_index.items()}\n",
    "lookup[0] = \"<PAD>\"\n",
    "lookup[1] = \"<START>\"\n",
    "lookup[2] = \"<UNK>\"\n",
    "lookup[4] = \"<UNUSED>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62ebff-2575-4d35-b46c-51c6f7598efc",
   "metadata": {},
   "source": [
    "Let's look at our first training document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aaefdf8-0a49-4bdb-8b40-55665283c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited <UNUSED> part they played and you\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([lookup[i] for i in S_train[0][:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e985a73-bfd9-42bd-a523-3dc6e223d602",
   "metadata": {},
   "source": [
    "We save this lookup table so it can be loaded later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d95252de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lookup, open('IMDB_word_index.pkl', 'bw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d900b9",
   "metadata": {},
   "source": [
    "## Padded representations\n",
    "\n",
    "For some of the recurrent models, we'll need sequences of common lengths, padded if necessary.\n",
    "Here, we pad up to a maximum length of 500, filling the remaining entries with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "637b3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(S_train,\n",
    " S_test) = [torch.tensor(pad_sequences(S, maxlen=500, value=0))\n",
    "            for S in [S_train,\n",
    "                      S_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6218300-b355-44cc-b7fb-4bff81211aa6",
   "metadata": {},
   "source": [
    "Finally, we save these for later use in the deep learning lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bac69f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(S_train, 'IMDB_S_train.tensor')\n",
    "torch.save(S_test, 'IMDB_S_test.tensor')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "source///ipynb,jupyterbook///md:myst,jupyterbook///ipynb",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

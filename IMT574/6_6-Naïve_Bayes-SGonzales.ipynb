{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e44314fa",
   "metadata": {},
   "source": [
    "# 6.6 Assignment 6: Naïve Bayes\n",
    "Bayes’ Theorem shows us how to turn P(E|H) to P(H|E), with E=Evidence and H=Hypothesis. But what does that really mean? Imagine you have to explain this to someone who doesn't understand machine learning or probability at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa00a2",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "<b>Question</b> Explain how to turn P(E|H) to P(H|E), with E=Evidence and H=Hypothesis in layman's terms.<p>\n",
    "<b>Answer</b>:<p>\n",
    "P(E|H) is the probability of seeing the Evidence if the Hypothesis is true. For example, the probability of getting a positive test result if you actually have the disease.<p>\n",
    "P(H|E) is the probability that the Hypothesis is true given that you've seen the Evidence. For example, the probability that you actually have the disease if you got a positive test result.<p>\n",
    "So, you turn P(E|H) into P(H|E) by incorporating the initial belief in the hypothesis P(H) and the overall likelihood of seeing the evidence P(E).<p>\n",
    "P(H) For example, before you take a test, the probability that someone in the general population has the disease. It's the probability that a randomly selected person from the general population has the disease before you have any specific information about that person's test result.<p>\n",
    "P(E) Represents the overall likelihood of seeing the evidence (a positive test) across the entire population, considering both people who have the disease and people who don't. This is the probability of the test itself, which is typically different than the actual results due to false negatives and false positives.<p>\n",
    "So, to convert P(E|H) into P(H∣E) using Bayes' Theorem:<p>\n",
    "$$P(H|E) = \\frac {P(E|H)×P(H)} {P(E)}$$<p>\n",
    "\n",
    "The example from real life is:<p>\n",
    " - There is a test for a cancer that occurs in 1% P(H) of the general population.\n",
    " - The test produces positive results 2% P(E) of the time.\n",
    " - If you actually have the cancer there is a 100% P(E|H) probability that the test will produce a positive result.\n",
    "So to turn P(E|H) into P(H|E):<p>\n",
    "$$P(H|E) = \\frac {1×.01} {.02}$$<p>\n",
    "$$P(H|E) = 50\\%$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c15fbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T05:34:56.917793Z",
     "start_time": "2025-05-11T05:34:56.903055Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46860235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T05:34:56.950307Z",
     "start_time": "2025-05-11T05:34:56.918770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['COMMENT_ID', 'AUTHOR', 'DATE', 'CONTENT', 'CLASS'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>z124fn5ahqnfdbxtg23ihlijyqjqtr1lk</td>\n",
       "      <td>Oh 1080s</td>\n",
       "      <td>2014-11-02T01:08:10</td>\n",
       "      <td>Sub my channel!﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>z12lubwrvv35zpzub23ywxbbiuawjbalc</td>\n",
       "      <td>Ariel Baptista</td>\n",
       "      <td>2014-11-02T05:06:46</td>\n",
       "      <td>http://www.ebay.com/itm/131338190916?ssPageNam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>z13osfxhtkfmwpxue234z3wimzmcs1k2x</td>\n",
       "      <td>Stefano Albanese</td>\n",
       "      <td>2014-11-02T12:04:36</td>\n",
       "      <td>http://www.guardalo.org/best-of-funny-cats-gat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>z12tcdxa5k3bsvtqh04ccnaqusj1vvfju3s</td>\n",
       "      <td>Salim Tayara</td>\n",
       "      <td>2014-11-02T14:33:30</td>\n",
       "      <td>if your like drones, plz subscribe to Kamal Ta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>z13gsp34uumcdje4q04civk54zr4wr0gxjc</td>\n",
       "      <td>Lucky D.</td>\n",
       "      <td>2014-11-02T14:53:06</td>\n",
       "      <td>http://hackfbaccountlive.com/?ref=4604617﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     COMMENT_ID            AUTHOR  \\\n",
       "0   LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1   LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2   LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3           z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4           z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "..                                          ...               ...   \n",
       "72            z124fn5ahqnfdbxtg23ihlijyqjqtr1lk          Oh 1080s   \n",
       "73            z12lubwrvv35zpzub23ywxbbiuawjbalc    Ariel Baptista   \n",
       "74            z13osfxhtkfmwpxue234z3wimzmcs1k2x  Stefano Albanese   \n",
       "75          z12tcdxa5k3bsvtqh04ccnaqusj1vvfju3s      Salim Tayara   \n",
       "76          z13gsp34uumcdje4q04civk54zr4wr0gxjc          Lucky D.   \n",
       "\n",
       "                   DATE                                            CONTENT  \\\n",
       "0   2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1   2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2   2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3   2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4   2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "..                  ...                                                ...   \n",
       "72  2014-11-02T01:08:10                                   Sub my channel!﻿   \n",
       "73  2014-11-02T05:06:46  http://www.ebay.com/itm/131338190916?ssPageNam...   \n",
       "74  2014-11-02T12:04:36  http://www.guardalo.org/best-of-funny-cats-gat...   \n",
       "75  2014-11-02T14:33:30  if your like drones, plz subscribe to Kamal Ta...   \n",
       "76  2014-11-02T14:53:06         http://hackfbaccountlive.com/?ref=4604617﻿   \n",
       "\n",
       "    CLASS  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "..    ...  \n",
       "72      1  \n",
       "73      1  \n",
       "74      1  \n",
       "75      1  \n",
       "76      1  \n",
       "\n",
       "[77 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('Youtube01-Psy.csv').dropna()\n",
    "print(df.columns)\n",
    "display(df.head(77))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aac2dfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T05:34:56.966283Z",
     "start_time": "2025-05-11T05:34:56.952283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 350 entries, 0 to 349\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   COMMENT_ID  350 non-null    object\n",
      " 1   AUTHOR      350 non-null    object\n",
      " 2   DATE        350 non-null    object\n",
      " 3   CONTENT     350 non-null    object\n",
      " 4   CLASS       350 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 13.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b155ba53",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Build a spam filter with the Naïve Bayes approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482a16e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T05:36:18.846799Z",
     "start_time": "2025-05-11T05:36:18.806712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not-spam       0.93      0.96      0.95        27\n",
      "        spam       0.98      0.95      0.96        43\n",
      "\n",
      "    accuracy                           0.96        70\n",
      "   macro avg       0.95      0.96      0.96        70\n",
      "weighted avg       0.96      0.96      0.96        70\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[26  1]\n",
      " [ 2 41]]\n",
      "\n",
      "--- Example Prediction ---\n",
      "Non Spam Message: 'i think about billions of the views come from people who only wanted to check the view count'\n",
      "Predicted class: Not Spam\n",
      "\n",
      "Spam Message: 'WINNER! You've won a free Bitcoin! Claim your prize nnow!'\n",
      "Predicted class: Spam\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X = df['CONTENT'] # Features (the text)\n",
    "y = df['CLASS']   # 0 not spam, 1 spam\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "vectorizer = CountVectorizer(stop_words='english') # Remove common English stop words\n",
    "\n",
    "# Fit the vectorizer on the training data and transform it\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the fitted vectorizer\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train the model using the vectorized training data and labels\n",
    "model.fit(X_train_counts, y_train)\n",
    "y_pred = model.predict(X_test_counts)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['not-spam', 'spam']))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n--- Example Prediction ---\")\n",
    "sample_message_not_spam = [\"i think about billions of the views come from people who only wanted to check the view count\"]\n",
    "sample_message_spam = [\"WINNER! You've won a free Bitcoin! Claim your prize nnow!\"]\n",
    "\n",
    "# Vectorize the sample messages using the same vectorizer\n",
    "sample_message_not_spam_counts = vectorizer.transform(sample_message_not_spam)\n",
    "sample_message_spam_counts = vectorizer.transform(sample_message_spam)\n",
    "\n",
    "# Predict\n",
    "pred_not_spam = model.predict(sample_message_not_spam_counts)\n",
    "pred_spam = model.predict(sample_message_spam_counts)\n",
    "\n",
    "print(f\"Non Spam Message: '{sample_message_not_spam[0]}'\")\n",
    "print(f\"Predicted class: {'Spam' if pred_not_spam[0] == 1 else 'Not Spam'}\")\n",
    "\n",
    "print(f\"\\nSpam Message: '{sample_message_spam[0]}'\")\n",
    "print(f\"Predicted class: {'Spam' if pred_spam[0] == 1 else 'Not Spam'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84afbd7",
   "metadata": {},
   "source": [
    "The Model was built using the YouTube spam dataset for a Psy Video (a South Korean pop music star). CLASS 0 is not spam and CLASS 1 is spam. The data in the `CONTENT` column was run through a `CountVectorizer` to remove words like 'a', 'the' etc. Basically it keeps the meaningful words and discards words that don't really contribute to the filter either way.<p>\n",
    "Next the raw training text messages `X_train` are converted into a format that the `MultinomialNB` model can understand. It figures out all the words it needs to pay attention to (the vocabulary from the training data) and then creates a matrix where each row is a message and columns are words, filled with how many times each word appears in each message.<p>\n",
    "Then the \"word-to-number\" mapping learned from the training data is applied to the unseen test messages. It counts the occurrences of words that were present in the training vocabulary within each test message, creating a numerical representation `X_test_counts` for the test data that has the exact same structure (columns representing the same words) as the training data's numerical representation `X_train_counts`.<p>\n",
    "A `MultinomialNB` model is then fit with the training data.<p>\n",
    "Finally the test data is predicted to determine accuracy and the confusion matrix.<p>\n",
    "The last step provides unique sample data to the model that we know to represent each class to double check the model for accuracy.<p>\n",
    "This model is extremely accurate at 96%. The model miscategorized a single non spam message as spam and miscategorized two spam messages as not spam."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
